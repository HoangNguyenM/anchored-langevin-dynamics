import numpy as np
import scipy.special as sp
from joblib import Parallel, delayed
from matplotlib import pyplot as plt
import optimizers, prior_sample

def wasserstein(sample_size, target, sample, cutoff=0.01):
    """Calculate 1-dimensional Wasserstein distance between two sets of data
    Args:
        sample_size: the size of the data sets
        target: a set of points from target distribution,  generated by the respected quantile function.
        sample: the set of sampled points.
        cutoff: specify how much of the tail is cut off.
    """
    sample.sort()
    
    lower = int(sample_size * cutoff)
    upper = int(sample_size * (1-cutoff))
    
    target = target[lower:(upper+1)]
    sample = sample[lower:(upper+1)]
    w = np.square(np.abs(target-sample))
    
    return np.mean(w)

def sliced_wasserstein(sample_size, target, sample, d=2, cutoff=0.01):
# Calculate sliced Wasserstein distance for high dimensional data
    one_dim_metrics = [wasserstein(sample_size=sample_size, target=target, 
                                   sample=sample[:,i], cutoff=cutoff) for i in range(d)]
    return np.sqrt(np.mean(one_dim_metrics))

class Laplace():
    """Laplace distribution
    Args:
        mean = 0, std = 1 by default
        b: hyperparameter of Laplace distribution, sigma: covariance matrix
        x: the sample of data, have size (batch_size, sample_size, d)
    """
    def __init__(self, b=1/(2**0.5), d=1, sigma=np.identity(1)):
        self.b = b
        self.d = d
        self.sigma = sigma
        self.inv_sigma = np.linalg.inv(sigma)
        self.det_sigma = np.linalg.det(self.sigma)

    # pdf calculation (take -log for Langevin exponent) for d=1, output has size (batch_size, sample_size,)
    def get_fn_value_1d(self,x):
        f = np.log(2*self.b)+np.abs(x[:,:,0]/self.b)
        return f

    # pdf calculation (take -log for Langevin exponent) for d>1, output has size (batch_size, sample_size,)
    def get_fn_value_high_d(self,x):
        v = (2-self.d)/2
        product = np.matmul(np.matmul(np.transpose(x[...,None],(0,1,3,2)),self.inv_sigma),x[...,None])

        coef = 2/((2*np.pi)**(self.d/2) * self.det_sigma**0.5)
        term2 = (product/2)**(v/2)
        bessel = sp.kv(v,(2*product)**0.5)
        f = coef * term2 * bessel
        return -np.log(f)[:,:,0,0]

    # quantile calculation (1-dimension)
    def get_quantiles(self,sample_size):
        left_half = np.linspace(1/sample_size, 1/2, num=sample_size//2)
        right_half = np.linspace(1/2+1/sample_size, 1-1/sample_size, num=sample_size//2-1)
        left_quantiles = np.log(2*left_half) * self.b
        right_quantiles = -np.log(2-2*right_half) * self.b
        return np.concatenate((left_quantiles,right_quantiles))
    
def main(scale=1, step_size=0.1, iterations=200):
    # Define hyperparameters
    sample_size = 1000
    d = 2

    prior_method = "normal"
    std = 10**0.5

    target_method = "laplace"
    b = 1/(2**0.5)
    var = np.identity(d)
    # var[0,1] = 0.5
    # var[1,0] = 0.5
    wass_cutoff = 0.01

    batch_size = 200

    parallel = True
    
    # Define optimizers
    optimizers_list = []
    target_dist = Laplace(b=b, d=d, sigma=var)
    if d == 1:
        fn_value = target_dist.get_fn_value_1d
    elif d > 1:
        fn_value = target_dist.get_fn_value_high_d
    else:
        raise ValueError(f"Dimension {d} not supported.")

    optimizers_list.append(optimizers.LD_Gauss_smooth(fn_value=fn_value, 
                                                      batch_size=batch_size, scale=scale, step_size=step_size))
    optimizers_list.append(optimizers.anchored_LD_Gauss_smooth(fn_value=fn_value, 
                                                      batch_size=batch_size, scale=scale, step_size=step_size))
    # optimizers_list.append(optimizers.time_change_LD_Gauss_smooth(fn_value=fn_value, 
    #                                                   batch_size=batch_size, scale=scale, step_size=step_size))

    # Save optimizers names
    opt_num = len(optimizers_list)
    names = [optimizers_list[k].name for k in range(opt_num)]
    
    # Make prior sample data
    prior_sample = prior_sample.sample_prior(sample_size=sample_size, d=d, method=prior_method, std=std)

    # Make initial metric calculation
    target_quantiles = target_dist.get_quantiles(sample_size=sample_size)

    metric_result = [sliced_wasserstein(sample_size=sample_size, target=target_quantiles,
                                        sample=prior_sample, d=d, cutoff=wass_cutoff)]

    # Define training function for each optimizer
    def train(vect, optimizer, metric_result):
        for _ in range(iterations):
            grad = optimizer.get_ref_grad(vect)
            vect += optimizer.update_step(vect, grad)

            metric_result.append(sliced_wasserstein(sample_size=sample_size, target=target_quantiles,
                                                    sample=vect, d=d, cutoff=wass_cutoff))
        return metric_result
    
    if parallel:
        # Parallelize the training process by optimizers
        print("____Start training process____")
        pool = Parallel(n_jobs = opt_num, backend = 'loky', verbose = 51, pre_dispatch = 'all')
        results = pool(delayed(train)(vect=prior_sample.copy(), optimizer=optimizers_list[k], 
                                    metric_result=metric_result.copy()) for k in range(opt_num))
    else:
        # Run the training process without parallelization
        results = []
        for k in range(opt_num):
            print(f"Current optimizer is {names[k]}")
            results.append(train(vect=prior_sample.copy(), optimizer=optimizers_list[k], 
                                    metric_result=metric_result.copy()))
        
    return results, names

### The main code ###

scales_list = [0.1, 0.5]
step_sizes_list = [0.01] * len(scales_list)
iterations = 500
epsilon = 0.2

# For single scale test
if len(scales_list) < 2:
    results, names = main(scale=scales_list[0], step_size=step_sizes_list[0], iterations=iterations)

    x = [i for i in range(len(results[0]))]
    for i in range(len(results)):
        plt.plot(x, results[i], label = names[i])
    plt.ylabel('Wasserstein distance')
    plt.xlabel('Iteration')
    plt.legend()
    plt.show()

# For multiple scale test
else:
    results_list = []
    names_list = []

    for i in range(len(scales_list)):
        results, names = main(scale=scales_list[i], step_size=step_sizes_list[i], iterations=iterations)
        results_list.append(results)
        names_list.append(names)

    for j in range(len(results_list[0])):
        for i in range(len(scales_list)):
            plt.plot([i for i in range(len(results_list[i][j]))], results_list[i][j], 
                     label = names_list[i][j] + ' @ scales=' + str(scales_list[i]) + ', lr=' + str(step_sizes_list[i]))
    plt.axhline(y = epsilon, linestyle = 'dashed', label = 'y = ' + str(epsilon)) 

    plt.ylabel('Wasserstein distance')
    plt.xlabel('Iteration')
    plt.legend()
    plt.show()
import numpy as np
import scipy.special as sp
from joblib import Parallel, delayed
from matplotlib import pyplot as plt
import optimizers, prior_sample

def wasserstein(sample_size, target, sample, cutoff=0.01):
    """Calculate 1-dimensional Wasserstein distance between two sets of data
    Args:
        sample_size: the size of the data sets
        target: a set of points from target distribution,  generated by the respected quantile function.
        sample: the set of sampled points.
        cutoff: specify how much of the tail is cut off.
    """
    sample.sort()
    
    lower = int(sample_size * cutoff)
    upper = int(sample_size * (1-cutoff))
    
    target = target[lower:(upper+1)]
    sample = sample[lower:(upper+1)]
    w = np.square(np.abs(target-sample))
    
    return np.mean(w)

def sliced_wasserstein(sample_size, target, sample, d=2, cutoff=0.01):
# Calculate sliced Wasserstein distance for high dimensional data
    one_dim_metrics = [wasserstein(sample_size=sample_size, target=target, 
                                   sample=sample[:,i], cutoff=cutoff) for i in range(d)]
    return np.sqrt(np.mean(one_dim_metrics))

class Laplace():
    """Laplace distribution
    Args:
        mean = 0, std = 1 by default
        b: hyperparameter of Laplace distribution, sigma: covariance matrix
        x: the sample of data, have size (simulation_num, sample_size, d)
    """
    def __init__(self, b=1/(2**0.5), d=1, sigma=np.identity(1)):
        self.b = b
        self.d = d
        self.sigma = sigma
        self.inv_sigma = np.linalg.inv(sigma)
        self.det_sigma = np.linalg.det(self.sigma)

    # pdf calculation (take -log for Langevin exponent) for d=1, output has size (simulation_num, sample_size,)
    def get_fn_value_1d(self,x):
        f = np.log(2*self.b)+np.abs(x[:,:,0]/self.b)
        return f

    # pdf calculation (take -log for Langevin exponent) for d>1, output has size (simulation_num, sample_size,)
    def get_fn_value_high_d(self,x):
        v = (2-self.d)/2
        product = np.matmul(np.matmul(np.transpose(x[...,None],(0,1,3,2)),self.inv_sigma),x[...,None])

        coef = 2/((2*np.pi)**(self.d/2) * self.det_sigma**0.5)
        term2 = (product/2)**(v/2)
        bessel = sp.kv(v,(2*product)**0.5)
        f = coef * term2 * bessel
        return -np.log(f)[:,:,0,0]

    # quantile calculation (1-dimension)
    def get_quantiles(self,sample_size):
        left_half = np.linspace(1/sample_size, 1/2, num=sample_size//2)
        right_half = np.linspace(1/2+1/sample_size, 1-1/sample_size, num=sample_size//2-1)
        left_quantiles = np.log(2*left_half) * self.b
        right_quantiles = -np.log(2-2*right_half) * self.b
        return np.concatenate((left_quantiles,right_quantiles))
    
def run(sample_size = 5000, simulation_num = 200, 
        d = 2, scale = 1, step_size = 0.1, iterations = 200, 
        prior_method = "normal", prior_std = 2, 
        target_method = "laplace", b = 1, var_bool = False, 
        wass_cutoff = 0.01, parallel = True):
    
    """Generate Laplace distribution
    Args:
        sample_size: the number of points generated
        simulation_num: the number of Monte Carlo simulation for Gaussian smoothing
        d: dimension of the data generated
        prior_method: prior data distribution, mean = 0 by default
        prior_std: std of the prior data distribution
        target_method: the target distribution to be generated, only Laplace supported
        b: hyperparameter of Laplace distribution
        parallel: if True, run parallel over CPU cores
    """

    # make variance matrix
    var = np.identity(d)
    if var_bool:
        var[0,1] = 0.5
        var[1,0] = 0.5
        
    # Define optimizers
    optimizers_list = []
    target_dist = Laplace(b=b, d=d, sigma=var)
    if d == 1:
        fn_value = target_dist.get_fn_value_1d
    elif d > 1:
        fn_value = target_dist.get_fn_value_high_d
    else:
        raise ValueError(f"Dimension {d} not supported.")

    optimizers_list.append(optimizers.LD_Gauss_smooth(fn_value=fn_value, 
                                                      simulation_num=simulation_num, scale=scale, step_size=step_size))
    optimizers_list.append(optimizers.anchored_LD_Gauss_smooth(fn_value=fn_value, 
                                                      simulation_num=simulation_num, scale=scale, step_size=step_size))
    # uncomment to add time change Langevin optimizer
    #optimizers_list.append(optimizers.time_change_LD_Gauss_smooth(fn_value=fn_value, 
    #                                                  simulation_num=simulation_num, scale=scale, step_size=step_size))

    # Save optimizers names
    opt_num = len(optimizers_list)
    names = [optimizers_list[k].name for k in range(opt_num)]
    
    # Make prior sample data
    _prior_sample = prior_sample.sample_prior(sample_size=sample_size, d=d, method=prior_method, std=prior_std)

    # Make initial metric calculation
    target_quantiles = target_dist.get_quantiles(sample_size=sample_size)

    metric_result = [sliced_wasserstein(sample_size=sample_size, target=target_quantiles,
                                        sample=_prior_sample, d=d, cutoff=wass_cutoff)]

    # Define training function for each optimizer
    def train(vect, optimizer, metric_result):
        for _ in range(iterations):
            grad = optimizer.get_ref_grad(vect)
            vect += optimizer.update_step(vect, grad)

            metric_result.append(sliced_wasserstein(sample_size=sample_size, target=target_quantiles,
                                                    sample=vect, d=d, cutoff=wass_cutoff))
        return metric_result
    
    if parallel:
        # Parallelize the training process by optimizers
        print("____Start training process____")
        pool = Parallel(n_jobs = opt_num, backend = 'loky', verbose = 51, pre_dispatch = 'all')
        results = pool(delayed(train)(vect=_prior_sample.copy(), optimizer=optimizers_list[k], 
                                    metric_result=metric_result.copy()) for k in range(opt_num))
    else:
        # Run the training process without parallelization
        results = []
        for k in range(opt_num):
            print(f"Current optimizer is {names[k]}")
            results.append(train(vect=_prior_sample.copy(), optimizer=optimizers_list[k], 
                                    metric_result=metric_result.copy()))
        
    return results, names

### The main code ###
if __name__ == "__main__":
    # Define hyperparameters
    sample_size = 1000
    d = 2

    prior_method = "normal"
    prior_std = 10**0.5

    target_method = "laplace"
    b = 1/(2**0.5)

    scales_list = [0.1, 0.5]
    step_sizes_list = [0.01] * len(scales_list)
    iterations = 500
    epsilon = 0.2

    # For single scale test
    if len(scales_list) < 2:
        results, names = run(sample_size=sample_size, d=2, 
                             scale=scales_list[0], step_size=step_sizes_list[0], iterations=iterations, 
                             prior_method=prior_method, prior_std=prior_std,
                             target_method=target_method, b=b, var_bool=False)

        x = [i for i in range(len(results[0]))]
        for i in range(len(results)):
            plt.plot(x, results[i], label = names[i])
        plt.ylabel('Wasserstein distance')
        plt.xlabel('Iteration')
        plt.legend()
        plt.show()

    # For multiple scale test
    else:
        results_list = []
        names_list = []

        for i in range(len(scales_list)):
            results, names = run(sample_size=sample_size, d=2, 
                                scale=scales_list[i], step_size=step_sizes_list[i], iterations=iterations, 
                                prior_method=prior_method, prior_std=prior_std,
                                target_method=target_method, b=b, var_bool=False)
            results_list.append(results)
            names_list.append(names)

        for j in range(len(results_list[0])):
            for i in range(len(scales_list)):
                plt.plot([i for i in range(len(results_list[i][j]))], results_list[i][j], 
                        label = names_list[i][j] + ' @ scales=' + str(scales_list[i]) + ', lr=' + str(step_sizes_list[i]))
        plt.axhline(y = epsilon, linestyle = 'dashed', label = 'y = ' + str(epsilon)) 

        plt.ylabel('Wasserstein distance')
        plt.xlabel('Iteration')
        plt.legend()
        plt.show()